{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def write_ply(fn, verts, colors): \n",
    "    ply_header = '''ply\n",
    "    format ascii 1.0\n",
    "    element vertex %(vert_num)d\n",
    "    property float x\n",
    "    property float y\n",
    "    property float z\n",
    "    property uchar red\n",
    "    property uchar green\n",
    "    property uchar blue\n",
    "    end_header\n",
    "    '''\n",
    "    verts = verts.reshape(-1, 3)\n",
    "    colors = colors.reshape(-1, 3)\n",
    "    verts = np.hstack([verts, colors])\n",
    "    with open(fn, 'wb') as f:\n",
    "        f.write((ply_header % dict(vert_num=len(verts))).encode('utf-8'))\n",
    "        np.savetxt(f, verts, fmt='%f %f %f %d %d %d ')\n",
    "        \n",
    "img_dir = './imgs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8b8367",
   "metadata": {},
   "source": [
    "---\n",
    "#### P6: Stereo images\n",
    "\n",
    "\n",
    "---\n",
    "<div class=\"alert alert-info\">\n",
    "<p>\n",
    "University of Applied Sciences Munich<br>\n",
    "Dept of Electrical Enineering and Information Technology<br>\n",
    "Institute for Applications of Machine Learning and Intelligent Systems (IAMLIS)<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(c) Alfred Sch√∂ttl 2024<br>\n",
    "    \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "In this notebook, we analyze rectified stereo images and compute the depth of certain image areas based on disparities. Parts of the tutorial are based on the official OpenCV documentation, see samples, see https://github.com/opencv/opencv/blob/master/samples/python/stereo_match.py .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86712fad",
   "metadata": {},
   "source": [
    "### 1. Acquire rectified images\n",
    "We have already prepared __*rectified images*__ for you. These are obtained by two cameras in a parallel stereo view configuration. Alternatively, you could use two cameras in a general configuration, measure their relative pose by a __*stereo calibration*__ procedure and warp the images so that the epipolar lines are horizontal lines.\n",
    "\n",
    "Plot the two images side by side and compare them.\n",
    "\n",
    "_Hint_: Use `axs[0].imshow` and do not switch off the axes for a better comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3032396",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgL = cv2.imread(os.path.join(img_dir, 'left-aloe-image-1.png'))\n",
    "imgR = cv2.imread(os.path.join(img_dir, 'right-aloe-image-2.png'))\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc8f59b",
   "metadata": {},
   "source": [
    "### 2. Find disparities\n",
    "There are various ways to find corresponding image points. One is to rely on eminent imagefeatures points based on methods like SIFT, SURF, HOG. Another possibility, especially for rectified images, is __*block matching*__. We simply scan on the epipolar line for the corresponding, best matching image point. Read the documentation of `StereoSGBM_create` and `compute` to understand how to perform block matching.\n",
    "\n",
    "_Hint:_ To display the disparities, scale the `disp` image values between 0 and 1. The minimum disparity is 0, the maximum is `num_disp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502758e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look into the documentation and try different parameters to improve the block matching result\n",
    "window_size = 3\n",
    "min_disp = 0\n",
    "num_disp = 80-min_disp\n",
    "stereo = cv2.StereoSGBM_create(minDisparity = min_disp, \n",
    "    numDisparities = num_disp,\n",
    "    blockSize = 16,\n",
    "    P1 = 8*3*window_size**2,\n",
    "    P2 = 32*3*window_size**2,\n",
    "    disp12MaxDiff = 1,\n",
    "    uniquenessRatio = 10,\n",
    "    speckleWindowSize = 100,\n",
    "    speckleRange = 32\n",
    ")\n",
    "\n",
    "disp = stereo.compute(imgL, imgR)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91d36e",
   "metadata": {},
   "source": [
    "### 3. Generate a point cloud\n",
    "The function `reprojectImageTo3D(d,Q)` takes the image of disparities and computes the depths of all pixels based on a matrix $Q$ (called _disparity to 3D matrix_) which needs to be known or estimated. \n",
    "\n",
    "Let $\\tilde p=(x,y,d,1)$ be the homogeneous vector consisting of the image coordinates $x,y$ of a feature and its disparity $d$. To make the formulas independent of the baseline length, let the disparity be given in units of the baseline length, $d/b$. Check that our formulas for the parallel-view reconstruction can essentially be written as $Q \\tilde p$ with the 4x4 matrix $Q$ as described below.  \n",
    "\n",
    "A good guess for the focal length $f$ of our cameras is 250 (pixels). \n",
    "\n",
    "_Hint_: Pixels whose depths could not determined are assigned the depth `np.inf` (infinity) or -1. We will remove these pixels from the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5650da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = imgL.shape[:2]\n",
    "f = 250\n",
    "Q = np.array([[1, 0, 0, -0.5*w],\n",
    "              [0,-1, 0,  0.5*h], # turn points 180 deg around x-axis,\n",
    "              [0, 0, 0,     -f], # so that the y-axis looks up\n",
    "              [0, 0, 1,      0]])\n",
    "\n",
    "points = cv2.reprojectImageTo3D(disp, Q)\n",
    "\n",
    "colors = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)\n",
    "mask = disp > -1\n",
    "out_points, out_colors = points[mask], colors[mask]\n",
    "# just for safety: remove all infinity entries\n",
    "I = ~np.isinf(out_points).any(axis=1)\n",
    "out_points, out_colors = out_points[I], out_colors[I]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec61e9",
   "metadata": {},
   "source": [
    "### 4. Render the result\n",
    "We store the point cloud in a widely used file format (ply). Try a point cloud viewer such as meshlab, https://www.meshlab.net/ (free and available for all OSs), just load the .ply file. Additionally, you can try to render the point cloud with the matplotlib 3D viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_ply('out.ply', out_points, out_colors)     # you may use this file later for visualization (see below)\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.axes(projection='3d')\n",
    "N = out_points.shape[0]\n",
    "I = np.random.randint(0, high=N-1, size=int(N*0.05))   # subsample to accelerate the rendering, increase the rate\n",
    "x, y, z = out_points[I,0], out_points[I,1], out_points[I,2],\n",
    "plt.gca().scatter(x, y, z, c=z, marker=\".\", cmap=\"viridis\");\n",
    "# you will obtain a better feeling for the result using the ply file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f81e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
